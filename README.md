# RephrasingProject
Of course. Here is a `README.md` file for your project, incorporating an overview, results analysis, and key points from the research paper.

***

# Semantic Coverage Relationship Modeling

[cite_start]This project implements the framework described in the paper "Same, More, or Different? Classifying Semantic Coverage with Generative and Discriminative Models"[cite: 1]. It provides a comprehensive pipeline for generating a synthetic dataset to study Semantic Coverage Relationships (SCR), evaluating various Large Language Models (LLMs) on a controlled rephrasing task, and training classifiers to identify these relationships.

## Project Overview

[cite_start]The core idea is to model the semantic relationship between two documents by analyzing the answerability of questions across them[cite: 6]. This project operationalizes this concept through two main components:

1.  [cite_start]**`main.py` - Rephrasing & Evaluation Pipeline:** This script uses state-of-the-art LLMs (Llama-3.2, GPT-4.1, DeepSeek-R1) to rewrite text passages from a source dataset (like SQuAD)[cite: 7]. The rewriting is guided by specific instructions to **preserve** the information needed to answer one set of questions while simultaneously **omitting** the information for another set. The quality of these rewrites is then rigorously evaluated using a suite of metrics, including:
    * Semantic Similarity (BERTScore, SBERT)
    * Lexical Similarity (Jaccard)
    * LLM-based Judgment (Quality & Correctness)
    * **Rejection Accuracy**: A custom metric to measure how successfully a model removes specified information.

2.  **`classification.py` - SCR Classification:** Using the dataset generated by `main.py`, this script trains and evaluates models for the task of SCR classification. [cite_start]It defines three primary relationship types and an additional one for robustness[cite: 3, 4, 5]:
    * **Paraphrasing**: Both texts contain the same information.
    * **Inclusion (A includes B)**: Text A contains all the information in Text B, plus more.
    * **Inclusion (B includes A)**: Text B contains all the information in Text A, plus more.
    * **Mutual Exclusion**: The texts contain distinct, non-overlapping information.

    [cite_start]The script benchmarks both a fine-tuned BERT-based classifier and a generative model (GPT-4.1) on this classification task[cite: 8].

## Core Concepts from the Research Paper

[cite_start]This project is built upon the theoretical framework for Semantic Coverage Relationships (SCR)[cite: 2]. Key concepts include:

* **Defining Relationships via QA:** The core innovation is using question answerability as a practical method to define and verify semantic relationships. [cite_start]If Document B can answer all the questions of Document A (and more), then B includes A. If they answer the same set of questions, they are paraphrases[cite: 41].
* **Three Core Relationships:**
    1.  [cite_start]**Paraphrase (P):** The same information is presented in different forms[cite: 3].
    2.  [cite_start]**Inclusion (I):** One document fully contains the information of another and adds new details[cite: 4].
    3.  [cite_start]**Mutual Exclusion (E):** The documents present distinct, non-overlapping content[cite: 5].
* [cite_start]**Practical Applications:** Understanding SCR is critical for a wide range of NLP tasks, including multi-document summarization, information retrieval, fact-checking, and educational technology[cite: 12, 11, 13, 15].
* [cite_start]**Dataset and Evaluation:** The paper proposes creating a synthetic dataset for SCR by paraphrasing and selectively omitting information from an existing corpus like SQuAD[cite: 7]. [cite_start]It also compares the effectiveness of generative models (via prompting) and discriminative models (fine-tuned BERT) for classifying these relationships[cite: 55].

## Results & Analysis

The experiments yielded several key insights into the capabilities of modern LLMs for controlled text generation and semantic classification.

### Rephrasing Task Performance

A comparison of the three models on the primary rephrasing task reveals a trade-off between semantic fidelity and the ability to follow complex negative constraints (i.e., omitting information).

| Model | BERTScore F1 | SBERT Similarity | Rejection Accuracy | Judge: Rephrase Quality | Judge: Answer Correctness | Jaccard Token |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **GPT-4.1** | **0.921** | **0.851** | 0.742 | **4.285** | **4.673** | **0.357** |
| **Llama-3.2-3B-Instruct** | 0.887 | 0.711 | **0.815** | 4.269 | 4.106 | 0.328 |
| **DeepSeek-R1** | 0.899 | 0.807 | 0.048 | 4.252 | 4.360 | 0.253 |

*Summary derived from the model comparison report.*

**Key Takeaways:**

* **GPT-4.1** excelled at producing high-quality, semantically faithful paraphrases that were judged to be the most accurate. Its Jaccard score indicates it performed significant rewriting rather than simple copy-pasting.
* **Llama-3.2-3B-Instruct** demonstrated the highest **Rejection Accuracy**, making it the most effective model at the challenging task of omitting specified information while rewriting the context.
* **DeepSeek-R1** struggled significantly with the information omission requirement, achieving a very low Rejection Accuracy of just 4.8%. This suggests it did not adhere well to the negative constraints in the prompt.

### Rejection Classification Analysis

This analysis measures how reliably an LLM can make a piece of information "unanswerable" in the rephrased text.

| Class | Precision | Recall | F1-Score |
| :--- | :--- | :--- | :--- |
| **Should be Unanswerable (Omitted)** | 0.93 | 0.53 | 0.67 |
| **Should be Answerable (Kept)** | 0.67 | **0.96** | 0.79 |
| **Overall Accuracy** | | | **0.74** |

*Summary from the rejection classification report.*

**Key Takeaways:**

* The models are highly effective at ensuring that "kept" questions remain answerable, achieving a recall of **96%**.
* However, they are far less successful at making "omitted" questions truly unanswerable, with a recall of only **53%**. This indicates significant "information leakage," where clues related to the omitted topic remain in the rewritten text.
* The high precision (93%) for the "Unanswerable" class means that when a model *does* predict a question is unanswerable, it is very likely correct.

### SCR Classification Performance

The `classification.py` script trains and evaluates models on their ability to classify the relationship between the original and rewritten texts. This tests the models' capacity to understand nuanced semantic differences. The project benchmarks both fine-tuned discriminative models (BERT, RoBERTa) and prompted generative models (GPT-4.1).

The results from this stage would provide insights into which architectural approach is better suited for reasoning about the complex relationships of paraphrase, inclusion, and exclusion.

## How to Use

1.  **Setup:**
    * Install dependencies: `pip install -r requirements.txt`
    * Set up your environment variables in a `.env` file (e.g., `AZURE_AI_API_KEY`).

2.  **Data Generation:**
    * Place your source data (in SQuAD or the custom JSON format) in the `./data/` directory.
    * Configure the `LLM_CONFIGURATIONS` and other parameters in `main.py`.
    * Run the script: `python main.py`
    * This will produce result files like `enhanced_results_500_texts.json` and plots in the `rephrasing_plots_azure_ai_inf/` directory.

3.  **SCR Classification:**
    * Ensure the output JSON from `main.py` is correctly referenced in `classification.py`.
    * Run the script: `python classification.py`
    * This will train the BERT-based models and evaluate all classifiers, saving the results and plots.
